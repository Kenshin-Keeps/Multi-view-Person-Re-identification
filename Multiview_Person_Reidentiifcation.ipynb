{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from keras.layers import Layer\n","import numpy as np\n","import cv2\n","import warnings\n","from datetime import datetime\n","import tkinter as tk\n","from tkinter import filedialog\n","%matplotlib inline\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz\"\n","modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\"\n","# modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# gpus = tf.config.list_physical_devices('GPU')\n","# if gpus:\n","#     try:\n","#     # Currently, memory growth needs to be the same across GPUs\n","#         for gpu in gpus:\n","#             tf.config.experimental.set_memory_growth(gpu, True)\n","#         logical_gpus = tf.config.list_logical_devices('GPU')\n","#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","#     except RuntimeError as e:\n","#     # Memory growth must be set before GPUs have been initialized\n","#         print(e)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["h = 105\n","w = 105\n","def preprocess_image(filename):\n","    img_st = tf.io.read_file(filename)\n","    img1 = tf.image.decode_jpeg(img_st, channels=3)\n","    img1 = tf.image.convert_image_dtype(img1, tf.float32)\n","    img1 = tf.image.resize(img1, (h, w))\n","    return img1\n","\n","def preprocess_twin(inp_img, val_img):\n","    return (preprocess_image(inp_img), preprocess_image(val_img))\n","\n","class L1Dist(Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","    def call(self, inp_em, val_em):\n","        return tf.math.abs(inp_em-val_em)\n","\n","binary_cross_loss = tf.keras.losses.BinaryCrossentropy()\n","opt = tf.keras.optimizers.Adam(1e-4)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# def ForMatch():\n","#     a_img = [\"report\\\\target\\\\testv1_frame_69.jpg\"]*8\n","#     t_img = [\"report\\\\group\\\\new_lg\\\\testv1_frame_5.jpg\",\n","#             \"report\\\\group\\\\new_lg\\\\testv1_frame_64.jpg\",\n","#             \"report\\\\group\\\\new_lg\\\\testv1_frame_70.jpg\",\n","#             \"report\\\\group\\\\new_lg\\\\testv2_frame_38.jpg\"]\n","\n","#     anchor_img = tf.data.Dataset.from_tensor_slices(a_img)\n","#     test_img = tf.data.Dataset.from_tensor_slices(t_img)\n","\n","#     dataset = tf.data.Dataset.zip((anchor_img, test_img))\n","#     test_dataset = dataset.map(preprocess_twin)\n","#     test_dataset = test_dataset.batch(8, drop_remainder=False)\n","\n","#     siamese = tf.keras.models.load_model(\"weight\\\\FinalMatchTrainFET-1024-4L-fin.h5\", \n","#                                         custom_objects = {\"L1Dist\":L1Dist, \n","#                                                         \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})\n","\n","#     data = test_dataset.as_numpy_iterator()\n","#     test_inp, test_val = data.next()\n","#     yhat = siamese.predict([test_inp, test_val])\n","#     print(yhat)\n","#     # cv2.imwrite(f\"./report/result/{imgName[:len(imgName)-4]}__{yhat}.jpg\", cv2.imread(f\"./detected/{imgName}\"))\n","# ForMatch()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def checkForMatch(imgName):\n","    # a_img = [\"./testQ/t.jpg\"]\n","    a_img = [\"./testQ/t.jpg\"]\n","    t_img = [f\"./detected/{imgName}\"]\n","\n","    anchor_img = tf.data.Dataset.from_tensor_slices(a_img)\n","    test_img = tf.data.Dataset.from_tensor_slices(t_img)\n","\n","    dataset = tf.data.Dataset.zip((anchor_img, test_img))\n","    test_dataset = dataset.map(preprocess_twin)\n","    test_dataset = test_dataset.batch(1, drop_remainder=False)\n","\n","    siamese = tf.keras.models.load_model(\"weight/NewArch-lr-0.0001-one-bn-1024.h5\", \n","                                        custom_objects = {\"L1Dist\":L1Dist, \n","                                                        \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})\n","\n","    data = test_dataset.as_numpy_iterator()\n","    test_inp, test_val = data.next()\n","    yhat = siamese.predict([test_inp, test_val])\n","\n","    if yhat > 0.8:\n","        cv2.imwrite(f\"./result/{imgName[:len(imgName)-4]}__{yhat}.jpg\", cv2.imread(f\"./detected/{imgName}\"))\n","        print(yhat)\n","        return True\n","    else:\n","        return False"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-09T16:43:14.309362Z","iopub.status.busy":"2022-11-09T16:43:14.308611Z","iopub.status.idle":"2022-11-09T16:43:20.486881Z","shell.execute_reply":"2022-11-09T16:43:20.485998Z","shell.execute_reply.started":"2022-11-09T16:43:14.309265Z"},"trusted":true},"outputs":[],"source":["from fileinput import filename\n","import os\n","from tensorflow.python.keras.utils.data_utils import get_file\n","np.random.seed(123)\n","\n","# Create a detector class\n","class Detector:\n","    # init method\n","    def __init__(self):\n","        pass\n","\n","    # Read classes from file\n","    def read_classes(self, path):\n","        with open(path, 'r') as f:\n","            self.classList = f.read().splitlines()\n","        self.colorList = np.random.uniform(\n","            0, 255, size=(len(self.classList), 3))\n","\n","    # Download Model\n","    def download_model(self, url):\n","        self.fileName = os.path.basename(url)\n","        self.modelName = self.fileName[:self.fileName.index('.')]\n","\n","        self.model_dir = './models'\n","        os.makedirs(self.model_dir, exist_ok=True)\n","\n","        # get the model\n","        get_file(fname=self.fileName,\n","                 origin=url,\n","                 cache_dir=self.model_dir,\n","                 cache_subdir=self.modelName,\n","                 extract=True)\n","\n","    # Load Model\n","    def load_model(self):\n","        # print(\"Loading model... \"+self.modelName)\n","        print(\"Loading model... \")\n","        tf.keras.backend.clear_session()\n","        model_path = os.path.join(\n","            self.model_dir, self.modelName, self.modelName, \"saved_model\")\n","        # print(\"Model path: \"+model_path)\n","        self.model = tf.saved_model.load(model_path)\n","        print(\"Model loaded\")\n","\n","    # Retrieve bounding boxes and class names\n","    def createBoundingBoxes(self, image, threshold, timer):\n","        image_tensor = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image_tensor = tf.convert_to_tensor(image_tensor) #Changed image to image_tensor\n","        image_tensor = image_tensor[tf.newaxis, ...]\n","\n","        results = self.model(image_tensor)\n","\n","        bboxes = results['detection_boxes'][0].numpy()\n","        classesIndexes = results['detection_classes'][0].numpy().astype(\n","            np.int32)\n","        ClassScores = results['detection_scores'][0].numpy()\n","\n","        imH, imW, imgC = image.shape\n","        bboxind = tf.image.non_max_suppression(bboxes,\n","                                               ClassScores,\n","                                               max_output_size=20,\n","                                               iou_threshold=threshold,\n","                                               score_threshold=threshold)\n","        if len(bboxind) > 0:\n","            c = 0\n","            for i in bboxind:\n","                bbox = tuple(bboxes[i].tolist())\n","                classConfidence = round(ClassScores[i] * 100, 2)\n","                classInd = classesIndexes[i]\n","                classLabelText = self.classList[classInd]\n","                if classLabelText != \"person\":\n","                    continue\n","                c += 1\n","                classColor = self.colorList[classInd]\n","                displayText = \"{}: {}%\".format(\n","                    classLabelText, classConfidence)\n","                ymin, xmin, ymax, xmax = bbox\n","                ymin, xmin, ymax, xmax = int(\n","                    ymin * imH), int(xmin * imW), int(ymax * imH), int(xmax * imW)\n","                crop_img = image[ymin:ymax,xmin:xmax]\n","                print(crop_img.shape)\n","                imgName = f\"{timer}_{c}.jpg\"\n","                print(imgName)\n","                cv2.imwrite(f\"./detected/{imgName}\", crop_img)\n","                # _ = checkForMatch(imgName)\n","                if not checkForMatch(imgName):\n","                    continue\n","                cv2.rectangle(image, (xmin, ymin),\n","                              (xmax, ymax), classColor, 3)\n","\n","                cv2.imshow('Output Result', image)\n","                if cv2.waitKey(1) & 0xFF == ord('q'):\n","                        break\n","\n","                # cv2.putText(image,\n","                #             displayText,\n","                #             (xmin, ymin-10),\n","                #             cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","                #             classColor, 2)\n","        return image\n","\n","    # Predict person on image\n","    def predict(self, imagePath, threshold):\n","        width = 600\n","        height = 550\n","        dim = (width, height)\n","        if imagePath.endswith('.mp4'):\n","            cap = cv2.VideoCapture(imagePath)\n","\n","            frame_width = int(cap.get(3))\n","            frame_height = int(cap.get(4))\n","            frame_size = (frame_width, frame_height)\n","            \n","            # scale_percent = sf # percent of original size\n","            # width = int(cap.get(3) * scale_percent / 100)\n","            # height = int(cap.get(4) * scale_percent / 100)\n","\n","            out_vid = cv2.VideoWriter(\n","                \"result\"+'.mp4',\n","                cv2.VideoWriter_fourcc(*'mp4v'), 30,\n","                frame_size)\n","            timer = -1\n","            while(cap.isOpened()):\n","                ret, frame = cap.read()\n","                timer += 1\n","                if ret == True:\n","                    frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n","                    frame = self.createBoundingBoxes(frame, threshold, timer)\n","                    cv2.imwrite(\"./FullFrame/\" + str(timer) + \".jpg\", frame)\n","                    out_vid.write(frame)\n","\n","                    # cv2.imshow('Output Result', frame)\n","                    # if cv2.waitKey(1) & 0xFF == ord('q'):\n","                    #     break\n","\n","                else:\n","                    break\n","            out_vid.release()\n","            cap.release()\n","            # cv2.destroyAllWindows()\n","        else:\n","            image = cv2.imread(imagePath)\n","            image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","            timer = 0\n","            bbimage = self.createBoundingBoxes(image, threshold, timer)\n","            cv2.imwrite(\"./FullFrame/\" + self.modelName + \".jpg\", bbimage)\n","            cv2.imshow(\"Result\", bbimage)\n","            cv2.waitKey(0)\n","            cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def getTargetImage(filePath, typeOfTarget):\n","    width = 600\n","    height = 550\n","    dim = (width, height)\n","    \n","    if typeOfTarget==\"video\":\n","        # Create a VideoCapture object and read from input file\n","        cap = cv2.VideoCapture(filePath)\n","\n","        # scale_percent = sf # percent of original size\n","        # width = int(cap.get(3) * scale_percent / 100)\n","        # height = int(cap.get(4) * scale_percent / 100)\n","        # width = 700\n","        # height = 600\n","        # dim = (width, height)\n","        \n","        # Check if camera opened successfully\n","        if (cap.isOpened()== False):\n","            print(\"Error opening video file\")\n","        \n","        # Read until video is completed\n","        while(cap.isOpened()):\n","        # Capture frame-by-frame\n","            ret, frame = cap.read()\n","            if ret == True:\n","                frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n","            # Display the resulting frame\n","                cv2.imshow('Frame', frame)\n","                # Press Q on keyboard to exit\n","                if cv2.waitKey(25) & 0xFF == ord('q'):\n","                    break\n","                # Press F on keyboard to select Image region\n","                if cv2.waitKey(25) & 0xFF == ord('f'):\n","                    roi = cv2.selectROI(frame)\n","                    print(roi)\n","\n","                    x1,y1,x2,y2 = roi\n","                    if not x1 and not x2 and not y1 and not y2:\n","                        cv2.destroyAllWindows()\n","                    else:\n","                        crop_img = frame[y1:y1+y2,x1:x1+x2]\n","                        cv2.imwrite(\"./testQ/t.jpg\", crop_img)\n","\n","                    cv2.waitKey(100)\n","                    cv2.destroyAllWindows()\n","        # Break the loop\n","            else:\n","                break\n","        # When everything done, release\n","        # the video capture object\n","        cap.release()\n","        # Closes all the frames\n","        cv2.destroyAllWindows()\n","    else:\n","        frame = cv2.imread(filePath)\n","        frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n","        # scale_percent = sf # percent of original size\n","        # width = int(frame.shape[0] * scale_percent / 100)\n","        # height = int(frame.shape[1] * scale_percent / 100)\n","        # dim = (width, height)\n","        roi = cv2.selectROI(frame)\n","        print(roi)\n","\n","        x1,y1,x2,y2 = roi\n","        if not x1 and not x2 and not y1 and not y2:\n","            cv2.destroyAllWindows()\n","        else:\n","            crop_img = frame[y1:y1+y2,x1:x1+x2]\n","            cv2.imwrite(\"./testQ/t.jpg\", crop_img)\n","\n","        cv2.waitKey(0)\n","        cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# print(\"Chose type of Target:\")\n","# print(\"If Target is an image insert 0\")\n","# print(\"If Target is a video insert 1\")\n","# typeOfTarget = int(input(\"Insert type: \"))\n","# scaleFactor = int(input(\"Scaling Ratio: \"))\n","# if typeOfTarget == 0: typeOfTarget = \"Image\"\n","# else: typeOfTarget = \"Video\"\n","# print(typeOfTarget)\n","# print(scaleFactor)\n","# print(\"Chose type of Query:\")\n","# print(\"If Query is an image insert 0\")\n","# print(\"If Query is a video insert 1\")\n","# typeOfQuery = int(input(\"Insert type: \"))\n","# if typeOfQuery == 0: typeOfQuery = \"Image\"\n","# else: typeOfQuery = \"Video\"\n","# print(typeOfQuery)\n","# getTargetImage(typeOfTarget,scaleFactor)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Target Location: D:/Academic/4th Year/2k17/Thesis/Pendrive Contents/Program/target.mp4\n","Target Type: video\n","Query Location: D:/Academic/4th Year/2k17/Thesis/Pendrive Contents/Program/query.mp4\n","Query Type: video\n","(125, 137, 125, 372)\n","Downloading data from http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n","20520960/20518283 [==============================] - 24s 1us/step\n","Loading model... \n","Model loaded\n","(347, 108, 3)\n","91_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 2s 2s/step\n","[[0.8090524]]\n","(336, 107, 3)\n","92_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 219ms/step\n","(339, 106, 3)\n","93_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 226ms/step\n","(327, 115, 3)\n","95_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 229ms/step\n","[[0.95050764]]\n","(253, 108, 3)\n","96_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002033DD57F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 0s 214ms/step\n","(246, 105, 3)\n","99_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020354884F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 0s 198ms/step\n","[[0.8053517]]\n","(251, 106, 3)\n","100_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 251ms/step\n","[[0.89521706]]\n","(249, 110, 3)\n","101_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 195ms/step\n","(249, 109, 3)\n","102_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 206ms/step\n","[[0.8965118]]\n","(254, 110, 3)\n","103_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 188ms/step\n","[[0.917117]]\n","(258, 110, 3)\n","104_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 189ms/step\n","[[0.9228829]]\n","(257, 107, 3)\n","105_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 191ms/step\n","[[0.86488914]]\n","(249, 108, 3)\n","106_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 183ms/step\n","[[0.8869977]]\n","(254, 109, 3)\n","107_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 180ms/step\n","[[0.87884647]]\n","(267, 119, 3)\n","108_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 183ms/step\n","[[0.8635961]]\n","(268, 119, 3)\n","109_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 177ms/step\n","[[0.92228425]]\n","(267, 119, 3)\n","110_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 181ms/step\n","[[0.8622972]]\n","(280, 120, 3)\n","111_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 186ms/step\n","[[0.966811]]\n","(288, 117, 3)\n","112_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 2s 2s/step\n","[[0.98774606]]\n","(293, 118, 3)\n","113_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 5s 5s/step\n","[[0.99599683]]\n","(290, 119, 3)\n","114_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 2s 2s/step\n","[[0.9920569]]\n","(294, 121, 3)\n","115_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 303ms/step\n","[[0.9892146]]\n","(291, 121, 3)\n","116_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 276ms/step\n","[[0.991589]]\n","(289, 121, 3)\n","117_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 262ms/step\n","[[0.9911759]]\n","(292, 122, 3)\n","118_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 270ms/step\n","[[0.9872448]]\n","(294, 123, 3)\n","119_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 266ms/step\n","[[0.9889637]]\n","(293, 126, 3)\n","120_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 245ms/step\n","[[0.9697751]]\n","(288, 129, 3)\n","121_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 242ms/step\n","[[0.9440745]]\n","(291, 130, 3)\n","122_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 234ms/step\n","[[0.90688926]]\n","(304, 133, 3)\n","123_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 212ms/step\n","[[0.9215011]]\n","(305, 130, 3)\n","124_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 289ms/step\n","(307, 129, 3)\n","125_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 215ms/step\n","(312, 128, 3)\n","126_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 211ms/step\n","[[0.8862154]]\n","(317, 128, 3)\n","127_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 211ms/step\n","[[0.9082626]]\n","(320, 132, 3)\n","128_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 204ms/step\n","[[0.92984843]]\n","(325, 132, 3)\n","129_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 197ms/step\n","[[0.8648646]]\n","(330, 132, 3)\n","130_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 208ms/step\n","(329, 132, 3)\n","131_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 202ms/step\n","(328, 131, 3)\n","132_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 195ms/step\n","(328, 133, 3)\n","133_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 190ms/step\n","(330, 134, 3)\n","134_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 189ms/step\n","(336, 138, 3)\n","135_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 202ms/step\n","(336, 136, 3)\n","136_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 187ms/step\n","(338, 138, 3)\n","137_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 182ms/step\n","(339, 137, 3)\n","138_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 191ms/step\n","(338, 135, 3)\n","139_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 185ms/step\n","(335, 134, 3)\n","140_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 185ms/step\n","(333, 138, 3)\n","141_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 192ms/step\n","(334, 138, 3)\n","142_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 188ms/step\n","(333, 139, 3)\n","143_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 187ms/step\n","(331, 138, 3)\n","144_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 186ms/step\n","(331, 132, 3)\n","145_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 192ms/step\n","(334, 130, 3)\n","146_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 188ms/step\n","(338, 132, 3)\n","147_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 184ms/step\n","(339, 132, 3)\n","148_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 184ms/step\n","(339, 132, 3)\n","149_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 180ms/step\n","(339, 131, 3)\n","150_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 188ms/step\n","(339, 130, 3)\n","151_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 183ms/step\n","(339, 130, 3)\n","152_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 2s 2s/step\n","(338, 129, 3)\n","153_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 2s 2s/step\n","(337, 130, 3)\n","154_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 2s 2s/step\n","(333, 130, 3)\n","155_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 391ms/step\n","(331, 130, 3)\n","156_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 292ms/step\n","(331, 131, 3)\n","157_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 266ms/step\n","(332, 131, 3)\n","158_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 263ms/step\n","(334, 131, 3)\n","159_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 246ms/step\n","(334, 130, 3)\n","160_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 238ms/step\n","(334, 131, 3)\n","161_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 247ms/step\n","(334, 131, 3)\n","162_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 234ms/step\n","(331, 131, 3)\n","163_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 243ms/step\n","(334, 132, 3)\n","164_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 229ms/step\n","(330, 133, 3)\n","165_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 233ms/step\n","[[0.8032533]]\n","(329, 132, 3)\n","166_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 212ms/step\n","[[0.80681634]]\n","(329, 132, 3)\n","167_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 221ms/step\n","(329, 131, 3)\n","168_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 270ms/step\n","(329, 131, 3)\n","169_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 202ms/step\n","(330, 132, 3)\n","170_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 207ms/step\n","(330, 132, 3)\n","171_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 221ms/step\n","(330, 132, 3)\n","172_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 199ms/step\n","(328, 130, 3)\n","173_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 201ms/step\n","(328, 131, 3)\n","174_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 204ms/step\n","[[0.80017304]]\n","(327, 131, 3)\n","175_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 190ms/step\n","[[0.85027426]]\n","(327, 128, 3)\n","176_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 191ms/step\n","[[0.8828524]]\n","(329, 127, 3)\n","177_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 241ms/step\n","[[0.84588647]]\n","(326, 127, 3)\n","178_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 211ms/step\n","[[0.8831849]]\n","(323, 123, 3)\n","179_1.jpg\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","1/1 [==============================] - 0s 185ms/step\n","[[0.91041714]]\n"]},{"name":"stderr","output_type":"stream","text":["Exception in Tkinter callback\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Zahim\\anaconda3\\envs\\mllab\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n","    return self.func(*args)\n","  File \"<ipython-input-10-b6eee72e97d4>\", line 41, in get_link\n","    detector.predict(VideoPath, threshold)\n","  File \"<ipython-input-7-596e2dae6911>\", line 128, in predict\n","    frame = self.createBoundingBoxes(frame, threshold, timer)\n","  File \"<ipython-input-7-596e2dae6911>\", line 51, in createBoundingBoxes\n","    results = self.model(image_tensor)\n","  File \"c:\\Users\\Zahim\\anaconda3\\envs\\mllab\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 686, in _call_attribute\n","    return instance.__call__(*args, **kwargs)\n","  File \"c:\\Users\\Zahim\\anaconda3\\envs\\mllab\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n","    raise e.with_traceback(filtered_tb) from None\n","  File \"c:\\Users\\Zahim\\anaconda3\\envs\\mllab\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 55, in quick_execute\n","    inputs, attrs, num_outputs)\n","tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n","\n","2 root error(s) found.\n","  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[1,96,320,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n","\t [[{{node ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/block_1_expand_BN/FusedBatchNormV3}}]]\n","Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n","\n","\t [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/Reshape_41/_118]]\n","Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n","\n","  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[1,96,320,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n","\t [[{{node ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/block_1_expand_BN/FusedBatchNormV3}}]]\n","Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n","\n","0 successful operations.\n","0 derived errors ignored. [Op:__inference_restored_function_body_53056]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-10-b6eee72e97d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;31m#####################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# ----------------------- END OF GUI CODE --------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\Zahim\\anaconda3\\envs\\mllab\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import tkinter as tk\n","from tkinter import filedialog\n","\n","# -----------------------  GUI CODE  -----------------------------\n","def open_file1():\n","    file1_path = filedialog.askopenfilename()\n","    file1_label.config(text=file1_path)\n","\n","\n","def open_file2():\n","    file2_path = filedialog.askopenfilename()\n","    file2_label.config(text=file2_path)\n","\n","\n","def get_link():\n","    file1 = file1_label.cget(\"text\")\n","    file2 = file2_label.cget(\"text\")\n","    typeOfTarget = t_Type.get().lower()\n","    typeOfQuery = q_Type.get().lower()\n","    print(\"Target Location:\", file1)\n","    print(\"Target Type:\", typeOfTarget)\n","    print(\"Query Location:\", file2)\n","    print(\"Query Type:\", typeOfQuery)\n","    getTargetImage(file1, typeOfTarget)\n","    namePath = \"Required Files\\coco.names\"\n","    # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\"\n","    # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\"\n","    # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz\"\n","    # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\"\n","    # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\"\n","    # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20210210/centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz\"\n","    modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\"\n","    imagePath = file2\n","    VideoPath = file2\n","    threshold = 0.6\n","    detector = Detector()\n","    detector.read_classes(namePath)\n","    detector.download_model(modelURL)\n","    detector.load_model()\n","    if typeOfQuery == \"video\":\n","        detector.predict(VideoPath, threshold)\n","    else:\n","        detector.predict(imagePath, threshold)\n","    print(\"Completed\")\n","\n","root = tk.Tk()\n","root.geometry(\"800x450+300+150\")\n","root.title(\"File Selector GUI\")\n","\n","################ Target Select #################\n","\n","file1_button = tk.Button(root, text=\"Select Target File to Crop\", command=open_file1)\n","file1_button.pack()\n","\n","file1_label = tk.Label(root, text=\"No file selected\")\n","file1_label.pack()\n","\n","t_Label = tk.Label(root, text=\"Enter Target Type:\")\n","t_Label.pack()\n","\n","t_Type = tk.Entry(root)\n","t_Type.pack()\n","\n","############### Query Select ###################\n","\n","file2_button = tk.Button(root, text=\"Select Query File to Search in\", command=open_file2)\n","file2_button.pack()\n","\n","file2_label = tk.Label(root, text=\"No file selected\")\n","file2_label.pack()\n","\n","q_Label = tk.Label(root, text=\"Enter Query Type:\")\n","q_Label.pack()\n","\n","q_Type = tk.Entry(root)\n","q_Type.pack()\n","\n","#################### Submit Button ###################################\n","\n","submit_button = tk.Button(root, text=\"Submit\", command=get_link)\n","submit_button.pack()\n","\n","#####################\n","\n","root.mainloop()\n","\n","# ----------------------- END OF GUI CODE --------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import tkinter as tk\n","\n","# root = tk.Tk()\n","# root.geometry(\"800x450+300+150\")\n","# root.mainloop()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T16:43:20.488965Z","iopub.status.busy":"2022-11-09T16:43:20.488409Z"},"trusted":true},"outputs":[],"source":["# filePath = \"Required Files\\coco.names\"\n","# # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\"\n","# # modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20210210/centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz\"\n","# modelURL = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\"\n","# imagePath = file\n","# VideoPath = \"./query.mp4\"\n","# threshold = 0.6\n","# detector = Detector()\n","# detector.read_classes(filePath)\n","# detector.download_model(modelURL)\n","# detector.load_model()\n","# if typeOfQuery == \"Video\":\n","#     detector.predict(VideoPath, threshold, scaleFactor)\n","# else:\n","#     detector.predict(imagePath, threshold, scaleFactor)\n","# print(\"Completed\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import tkinter as tk\n","# from tkinter import filedialog\n","\n","# # -----------------------  GUI CODE  -----------------------------\n","\n","\n","# def open_file1():\n","#     file1_path = filedialog.askopenfilename()\n","#     file1_label.config(text=file1_path)\n","\n","\n","# def open_file2():\n","#     file2_path = filedialog.askopenfilename()\n","#     file2_label.config(text=file2_path)\n","\n","\n","# def get_link():\n","#     file1 = file1_label.cget(\"text\")\n","#     file2 = file2_label.cget(\"text\")\n","#     target = t_Type.get().lower()\n","#     query = q_Type.get().lower()\n","#     print(\"Target Location:\", file1)\n","#     print(\"Target Type:\", target)\n","#     print(\"Query Location:\", file2)\n","#     print(\"Query Type:\", query)\n","\n","\n","# root = tk.Tk()\n","# root.title(\"File Selector GUI\")\n","\n","# ########## Target Select #################\n","\n","# file1_button = tk.Button(root, text=\"Select Target File\", command=open_file1)\n","# file1_button.pack()\n","\n","# file1_label = tk.Label(root, text=\"No file selected\")\n","# file1_label.pack()\n","\n","# t_Label = tk.Label(root, text=\"Enter Target Type:\")\n","# t_Label.pack()\n","\n","# t_Type = tk.Entry(root)\n","# t_Type.pack()\n","\n","# ############### Query Select ###################\n","\n","# file2_button = tk.Button(root, text=\"Select Query File\", command=open_file2)\n","# file2_button.pack()\n","\n","# file2_label = tk.Label(root, text=\"No file selected\")\n","# file2_label.pack()\n","\n","# q_Label = tk.Label(root, text=\"Enter Query Type:\")\n","# q_Label.pack()\n","\n","# q_Type = tk.Entry(root)\n","# q_Type.pack()\n","\n","# #################### Submit Button ###################################\n","\n","# submit_button = tk.Button(root, text=\"Submit\", command=get_link)\n","# submit_button.pack()\n","\n","# #####################\n","\n","# root.mainloop()\n","\n","# # ----------------------- END OF GUI CODE --------------------------\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # getQueryImage(fname)\n","# print(\"Chose type of Target:\")\n","# print(\"If Target is an image insert 0\")\n","# print(\"If Target is a video insert 1\")\n","# typeOfTarget = int(input(\"Insert type: \"))\n","# scaleFactor = int(input(\"Scaling Ratio: \"))\n","# print(typeOfTarget)\n","# print(scaleFactor)\n","# print(\"Chose type of Query:\")\n","# print(\"If Query is an image insert 0\")\n","# print(\"If Query is a video insert 1\")\n","# typeOfQuery = int(input(\"Insert type: \"))\n","# print(typeOfQuery)\n","# getQueryImage(typeOfTarget)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.6 ('mllab')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"vscode":{"interpreter":{"hash":"43c05a3e19b01079ffeaf8db5e23f85b9dc48275fbce953df18c5336d9b3b7c5"}}},"nbformat":4,"nbformat_minor":4}
